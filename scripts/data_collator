# data_collator.py

import random
import torch

import random
import torch
import pandas as pd

class Strategy1DataCollator:
    def __init__(self, tokenizer, tfidf_matrix, tfidf_tokens, document_indices, lexicon_list, mask_prob=0.15):
        self.tokenizer = tokenizer
        self.tfidf_matrix = tfidf_matrix
        self.tfidf_tokens = tfidf_tokens
        self.mask_prob = mask_prob  # % des mots Ã  masquer
        self.document_indices = document_indices
        self.lexicon_list = set(lexicon_list)  # Convertir en ensemble pour une recherche rapide

    def __call__(self, examples):
        batch_input_ids = []
        batch_labels = []
        
        for example in examples:
            text = self.tokenizer.decode(example["input_ids"], skip_special_tokens=True).lower()
            doc_index = self.document_indices.get(tuple(example["input_ids"]), None)
            if doc_index is None:
                continue  
            
            tokens = self.tokenizer.tokenize(text)
            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)

            token_tfidf_scores = {
                word: self.tfidf_matrix[doc_index, self.tfidf_tokens.tolist().index(word)]
                for word in tokens if word in self.tfidf_tokens
            }

            if not token_tfidf_scores:
                continue

            num_tokens_to_mask = max(1, int(self.mask_prob * len(tokens)))
            top_tokens = sorted(token_tfidf_scores, key=token_tfidf_scores.get, reverse=True)[:num_tokens_to_mask]
            lexicon_tokens = [token for token in top_tokens if token in self.lexicon_list]

            masked_token_ids = token_ids.copy()
            labels = [-100] * len(tokens)
            masked_list = []
            token_positions = {token: [] for token in lexicon_tokens}

            for idx, token in enumerate(tokens):
                if token in token_positions:
                    token_positions[token].append(idx)

            masked_count = 0
            for token, positions in token_positions.items():
                if positions:
                    idx = random.choice(positions)
                    masked_token_ids[idx] = self.tokenizer.mask_token_id  
                    labels[idx] = token_ids[idx]
                    masked_count += 1
            
            rest_masked_count = masked_count
            if masked_count < num_tokens_to_mask:
                extra_tokens_to_mask = num_tokens_to_mask - masked_count
                top_tokens_2 = [
                    token for token in sorted(token_tfidf_scores, key=token_tfidf_scores.get, reverse=True)
                    if token not in self.lexicon_list and token not in masked_list
                ][:extra_tokens_to_mask]

                token_positions_2 = {token: [] for token in top_tokens_2}
                for idx, token in enumerate(tokens):
                    if token in token_positions_2:
                        token_positions_2[token].append(idx)

                for token, positions in token_positions_2.items():
                    if masked_count >= num_tokens_to_mask:
                        break
                    if positions:
                        idx = random.choice(positions)
                        masked_token_ids[idx] = self.tokenizer.mask_token_id  
                        labels[idx] = token_ids[idx]
                        masked_list.append(token)
                        masked_count += 1

          
            masked_token_ids = masked_token_ids[:512]
            labels = labels[:512]
            max_length = 512  
            masked_token_ids = masked_token_ids + [self.tokenizer.pad_token_id] * (max_length - len(masked_token_ids))
            labels = labels + [-100] * (max_length - len(labels))

            batch_input_ids.append(masked_token_ids)
            batch_labels.append(labels)

        input_ids_tensor = torch.tensor(batch_input_ids)
        labels_tensor = torch.tensor(batch_labels)

        if torch.cuda.is_available():
            input_ids_tensor = input_ids_tensor.to('cuda')
            labels_tensor = labels_tensor.to('cuda')

        return {
            "input_ids": input_ids_tensor,
            "labels": labels_tensor
        }


class Strategy2DataCollator:
    def __init__(self, tokenizer, tfidf_matrix, tfidf_tokens, document_indices, lexicon_list, mask_prob=0.15):
        self.tokenizer = tokenizer
        self.tfidf_matrix = tfidf_matrix
        self.tfidf_tokens = tfidf_tokens
        self.mask_prob = mask_prob  
        self.document_indices = document_indices
        self.lexicon_list = set(lexicon_list)  

    def __call__(self, examples):
        batch_input_ids = []
        batch_labels = []
        
        max_length = max(len(example["input_ids"]) for example in examples) 

        for example in examples:
            text = self.tokenizer.decode(example["input_ids"], skip_special_tokens=True).lower()
            
            doc_index = self.document_indices.get(tuple(example["input_ids"]), None)
            if doc_index is None:
                continue
            
            tokens = self.tokenizer.tokenize(text)
            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)

            token_tfidf_scores = {
                word: self.tfidf_matrix[doc_index, self.tfidf_tokens.tolist().index(word)]
                for word in tokens if word in self.tfidf_tokens
            }

            num_tokens_to_mask = max(1, int(self.mask_prob * len(tokens)))
            top_tokens = [token for token in sorted(token_tfidf_scores, key=token_tfidf_scores.get, reverse=True)
                          if token not in self.lexicon_list][:num_tokens_to_mask]

            masked_token_ids = token_ids.copy()
            labels = [-100] * len(tokens)

            token_positions = {token: [] for token in top_tokens}
            for idx, token in enumerate(tokens):
                if token in token_positions:
                    token_positions[token].append(idx)

            masked_count = 0
            for token, positions in token_positions.items():
                if masked_count >= num_tokens_to_mask:
                    break
                if positions:
                    idx = random.choice(positions)
                    masked_token_ids[idx] = self.tokenizer.mask_token_id
                    labels[idx] = token_ids[idx]
                    masked_count += 1
                    
            masked_token_ids = masked_token_ids[:512]
            labels = labels[:512]

            masked_token_ids += [self.tokenizer.pad_token_id] * (512 - len(masked_token_ids))
            labels += [-100] * (512 - len(labels))

            batch_input_ids.append(masked_token_ids)
            batch_labels.append(labels)

        input_ids_tensor = torch.tensor(batch_input_ids)
        labels_tensor = torch.tensor(batch_labels)

        if torch.cuda.is_available():
            input_ids_tensor = input_ids_tensor.to('cuda')
            labels_tensor = labels_tensor.to('cuda')

        return {
            "input_ids": input_ids_tensor,
            "labels": labels_tensor
        }
